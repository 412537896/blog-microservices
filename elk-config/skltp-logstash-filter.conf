#
# Copied from /Users/magnus/Documents/projects/Inera/git-bitbucket/logstash/logstash-config/config-files
#

#==============================================================================
# SKLTP logstash filter configuration.
#
# Usage: put this file in a logstash configuration directory together with
#   logstash configuration files for input and output sections, then run
#   logstash: logstash -f <PATH-TO-CONFIG-DIRECTORY>
#
#==============================================================================

filter {
    
    #--------------------------------------------------------------------------
    # Categorize events and fix things common for all events.
    #--------------------------------------------------------------------------
    # match on first line
    grok {
        match => [ "message", "%{TIMESTAMP_ISO8601:log-timestamp} %{WORD:log-level} %{GREEDYDATA:log-firstline}" ]
    }
    # set the logstash event timestamp to be the timestamp from the logfile,
    # otherwise logstash will use the timestamp from when it read the log-event
    # from file (which is not what we want since shipping of the logs might be delayed)
    date {
        # Note: (logstash-1.3.2) grok-pattern TIMESTAMP_ISO8601 for log-timestamp
        # is more forgiving than date-filters match pattern "ISO8601" - which
        # gives a parse error here for timestamps like: 2014-05-05 12:06:01,098
        # which is log4j's representation of ISO8601 for the "%d" pattern, see:
        # https://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PatternLayout.html
        # and http://joda-time.sourceforge.net/cal_iso.html
        #match => [ "log-timestamp", "ISO8601" ]        
        match => [ "log-timestamp", "yyyy-MM-dd HH:mm:ss,SSS" ]
        # timezone: will use the host timezone if not configured - which is the same timezone
        #   used for writing the log4j-logs in this case, so ok to omit.
        # @timestamp will be automatically converted to UTC time - which is how timestamps
        #   should be stored in Elasticsearch (i.e. UTC)
        
        # for reference to local time, and cross-reference with timestamps in logfiles, keep
        # the log-timestamp in local time
        #remove_field => [ "log-timestamp" ]
    }
    # determine if this is a tracking-event (for message tracking) or a regular log-event
    if [log-firstline] =~ /soi-toolkit.log$/ {
        mutate {
            add_field => [ "type", "tp-track" ]
        }
    }
    else {
        mutate {
            add_field => [ "type", "tp-log" ]
        }
    }
    
    #--------------------------------------------------------------------------
    # Handle tracking-event metadata: begin
    #--------------------------------------------------------------------------
    # Note: metadata defined by:
    #   https://code.google.com/p/soi-toolkit/source/browse/tags/soitoolkit-0.6.0/commons/components/commons-mule/src/main/java/org/soitoolkit/commons/mule/log/DefaultEventLogger.java
    # and extensions for the ExtraInfo-property-bag.
    if [type] == "tp-track" {
        ### IntegrationScenarioId= : NOT IN USE
        ### ContractId= : NOT IN USE
        # LogMessage=
        grok {
            match => [ "message", "^LogMessage=%{GREEDYDATA:logMessage}" ]
            tag_on_failure => [ "logMessageParseFailure" ]
        }
        # waypoint: DERIVED METADATA for logstash - does not exist in incoming logevent
        if [logMessage] == "msg-in" or [logMessage] == "msg-out"
                or [logMessage] == "req-in" or [logMessage] == "req-out"
                or [logMessage] == "resp-in" or [logMessage] == "resp-out"
                or [logMessage] == "xreq-in" or [logMessage] == "xresp-out" {
            mutate {
                add_field => [ "waypoint", "%{[logMessage]}" ]
            }
        }
        # ServiceImpl=
        grok {
            match => [ "message", "^ServiceImpl=%{GREEDYDATA:serviceImpl}" ]
            tag_on_failure => [ "serviceImplParseFailure" ]
        }
        # Host=
        grok {
            match => [ "message", "^Host=%{GREEDYDATA:hostId}" ]
            tag_on_failure => [ "hostIdParseFailure" ]
        }
        # ComponentId=
        grok {
            match => [ "message", "^ComponentId=%{GREEDYDATA:componentId}" ]
            tag_on_failure => [ "componentIdParseFailure" ]
        }
        # Endpoint=
        grok {
            match => [ "message", "^Endpoint=%{GREEDYDATA:endpoint}" ]
            tag_on_failure => [ "endpointParseFailure" ]
        }        
        # MessageId=
        grok {
            match => [ "message", "^MessageId=%{GREEDYDATA:messageId}" ]
            tag_on_failure => [ "messageIdParseFailure" ]
        }        
        # BusinessCorrelationId=
        grok {
            match => [ "message", "^BusinessCorrelationId=%{GREEDYDATA:businessCorrelationId}" ]
            tag_on_failure => [ "businessCorrelationIdParseFailure" ]
        }
        ### BusinessContextId= : NOT IN USE
        ### ExtraInfo= : is handled last since it contains extensions
        # Payload= : NOTE: do not try to extract payload - just find out if there is
        #  any payload present - to use further down for deciding if a separate debug-log-event
        #  shall be created containing the full logevent with the payload
        grok {
            match => [ "message", "^Payload=%{GREEDYDATA:payload}" ]
            tag_on_failure => [ "payloadParseFailure" ]
        }
        # Stacktrace= : NOTE: this can be tricky to parse - make a simple, best effort.
        #   This field is only present if there is a stacktrace.
        #   The standard soi-toolkit logger adds stacktrace rows using prefix: "\n\t at"
        if [log-level] == "ERROR" {
            grok {
                #match => [ "message", "^Stacktrace=%{GREEDYDATA:stacktrace}" ]
                # using Oniguruma regexp: http://www.geocities.jp/kosako3/oniguruma/doc/RE.txt
                match => [ "message", "(?<stacktrace>^Stacktrace=.*(\n\t\sat\s.*)*)" ]
                tag_on_failure => [ "stackTraceParseFailure" ]
            }
        }
        
        #---------------------------------------------------
        # ExtraInfo: begin
        #---------------------------------------------------
        
        #-------------------------------
        # vp-log specials: begin
        #-------------------------------
        if [componentId] =~ /vp-services/ {
            #-------------------------------
            # normalize x-prefixed headers from vp-log to soi-tk standard like all other (non-vp) logevents
            #-------------------------------        
            if [waypoint] == "xreq-in" {
                mutate {
                    update => [ "waypoint", "req-in" ]
                }
            }
            if [waypoint] == "xresp-out" {
                mutate {
                    update => [ "waypoint", "resp-out" ]
                }
            }
            
            # -senderid=
            grok {
                match => [ "message", "^-senderid=%{GREEDYDATA:senderid}"]
                tag_on_failure => ["senderidParseFailure"]
            }
            # -originalServiceconsumerHsaid=
            grok {
                match => [ "message", "^-originalServiceconsumerHsaid=%{GREEDYDATA:originalServiceconsumerHsaid}"]
                tag_on_failure => ["originalServiceconsumerHsaidParseFailure"]
            }
            # -rivversion=
            grok {
                match => [ "message", "^-rivversion=%{GREEDYDATA:rivversion}"]
                tag_on_failure => ["rivversionParseFailure"]
            }
            # -senderIpAdress=
            grok {
                match => [ "message", "^-senderIpAdress=%{GREEDYDATA:senderIpAdress}"]
                tag_on_failure => ["senderIpAdressParseFailure"]
            }
            # -wsdl_namespace=
            grok {
                match => [ "message", "^-wsdl_namespace=%{GREEDYDATA:tjansteinteraktion}"]
                tag_on_failure => ["tjansteinteraktionParseFailure"]
            }
            # -servicecontract_namespace=
            grok {
                match => [ "message", "^-servicecontract_namespace=%{GREEDYDATA:tjanstekontrakt}"]
                tag_on_failure => ["tjanstekontraktParseFailure"]
            }
            # -receiverid
            grok {
                match => [ "message", "^-receiverid=%{GREEDYDATA:receiverid}"]
                tag_on_failure => ["receiveridErrorParseFailure"]
            }
            # resp-out handling
            if [waypoint] == "resp-out" {
                # -time.producer
                grok {
                    match => [ "message", "^-time.producer=%{NUMBER:time_producer:int}" ]
                    tag_on_failure => ["time_producerParseFailure" ]
                }
                # -endpoint_url=
                grok {
                    match => [ "message", "^-endpoint_url=%{GREEDYDATA:endpoint_url}" ]
                    tag_on_failure => ["endpoint_urlParseFailure" ]
                }

                if [log-level] == "ERROR" {
                    # -sessionErrorDescription=
                    grok {
                        match => [ "message", "^-sessionErrorDescription=%{GREEDYDATA:sessionErrorDescription}" ]
                        tag_on_failure => [ "sessionErrorDescriptionParseFailure" ]
                    }
                    # -sessionErrorTechnicalDescription
                    grok {
                        match => [ "message", "^-sessionErrorTechnicalDescription=%{GREEDYDATA:sessionErrorTechnicalDescription}" ]
                        tag_on_failure => ["sessionErrorTechnicalDescriptionParseFailure"]
                    }
                }
            }            
        }
        #-------------------------------
        # vp-log specials: end
        #-------------------------------
        
        #---------------------------------------------------
        # ExtraInfo: end
        #---------------------------------------------------
    }
    #--------------------------------------------------------------------------
    # Handle tracking-event metadata: end
    #--------------------------------------------------------------------------
        
    
    #--------------------------------------------------------------------------
    # Payload logging: begin
    #--------------------------------------------------------------------------
    # When DEBUG level is configured for writing tracking logs,
    # then we might have a populated payload field.
    #
    # Requirements:
    # 1. We are only allowed to keep payloads for a short time-period due to regulatory
    #   requirements.
    # 2. We like to keep log-events without payloads for a longer period to find
    #   trends and extract statistics.
    # 3. There may be a need to separate access to payloads from access to the
    #   tracking logs without payload (for confidentiality reasons).
    # 
    # Design:
    # a) Clear the logevent to be used for statistics from the payload, then we
    #   can keep these logevents for as long as we like.
    # b) Clone the logevent (before clearing payload) into a separate type and
    #   send both the original and cloned event to Elasticsearch.
    # c) If required, the cloned event (containing the payload) can be separated
    #   into a special stream (either at the logstash-agent or at the central
    #   logstash-indexer) and sent to different Elasticsearch instances if needed.
    # d) Having a separate type makes it simple to delete the events containing
    #   payloads from Elasticsearch on a frequent basis.
    #--------------------------------------------------------------------------
    if [type] == "tp-track" and [payload] != "null" and [payload] =~ /.+/ {
        clone {
            # NOTE/BUG: add a new field containing the message to the cloned event
            # since removing/updating the original event's message field also affects
            # the cloned events message field - see comments below with issue links.
            add_field => [ "raw-logevent", "%{[message]}" ]
            clones => [ "tp-track-debug" ]
        }
    }
    # Clear the message-field (that may contain payload)
    #
    # NOTE/BUG: this does NOT work as expected in logstash 1.4.1: the statements
    # within the if-condition are applied to ALL events after a clone.
    # Ref:
    #   https://logstash.jira.com/browse/LOGSTASH-1476
    #   https://logstash.jira.com/browse/LOGSTASH-1477    
    if [type] == "tp-track" {
        mutate {
            remove_field => [ "message" ]
            #update => [ "message", "DELETED" ]
        }
    }
    mutate {
        remove_field => [ "payload" ]
    }
    #--------------------------------------------------------------------------
    # Payload logging: end
    #--------------------------------------------------------------------------
    
}
